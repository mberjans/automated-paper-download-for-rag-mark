#!/usr/bin/env python3
"""
Test the actual API versions of FOODB scripts
This tests the drop-in replacements for the original pipeline scripts
"""

import sys
import os
import json
import time
from pathlib import Path

def create_sample_jsonl_data():
    """Create sample JSONL data for testing the API scripts"""
    print("üìÑ Creating sample JSONL data...")
    
    sample_data = [
        {
            "text": "Quercetin is a flavonoid found in onions and apples. Studies show that quercetin has anti-inflammatory properties and may reduce blood pressure in hypertensive patients.",
            "doi": "10.1234/test1",
            "title": "Quercetin and Cardiovascular Health",
            "section": "abstract"
        },
        {
            "text": "Lycopene is a carotenoid that gives tomatoes their red color. Research indicates that lycopene consumption is associated with reduced prostate cancer risk through antioxidant mechanisms.",
            "doi": "10.1234/test2", 
            "title": "Lycopene and Cancer Prevention",
            "section": "introduction"
        }
    ]
    
    # Create input directory
    input_dir = Path("FOODB_LLM_pipeline/sample_input")
    input_dir.mkdir(exist_ok=True)
    
    input_file = input_dir / "test_pipeline_data.jsonl"
    
    with open(input_file, 'w') as f:
        for item in sample_data:
            f.write(json.dumps(item) + '\n')
    
    print(f"‚úÖ Created sample data: {input_file}")
    return str(input_file)

def test_simple_sentence_api_script():
    """Test the API version of simple sentence generation script"""
    print("\nüî¨ Testing 5_LLM_Simple_Sentence_gen_API.py")
    print("=" * 55)
    
    try:
        # Change to the FOODB_LLM_pipeline directory
        os.chdir("FOODB_LLM_pipeline")
        
        # Import the API script
        sys.path.insert(0, '.')
        import importlib.util
        
        spec = importlib.util.spec_from_file_location(
            "simple_sentence_api", 
            "5_LLM_Simple_Sentence_gen_API.py"
        )
        simple_sentence_api = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(simple_sentence_api)
        
        # Test the generate function
        test_text = "Curcumin is the active compound in turmeric that exhibits anti-inflammatory properties."
        
        print(f"üìù Testing simple sentence generation...")
        print(f"Input: {test_text}")
        
        start_time = time.time()
        result = simple_sentence_api.generate_simple_sentences(
            test_text, 
            simple_sentence_api.given_prompt1
        )
        end_time = time.time()
        
        print(f"‚úÖ Generated in {end_time - start_time:.2f}s")
        print(f"Output: {result}")
        
        # Test entity checking
        print(f"\nüß¨ Testing entity checking...")
        test_sentence = "Curcumin reduces inflammation."
        
        entities = simple_sentence_api.check_entities_present(
            test_sentence,
            simple_sentence_api.given_prompt2
        )
        
        print(f"Sentence: {test_sentence}")
        print(f"Entities: {entities}")
        
        os.chdir("..")
        return True
        
    except Exception as e:
        print(f"‚ùå Error testing API script: {e}")
        os.chdir("..")
        return False

def test_triple_extraction_api_script():
    """Test the API version of triple extraction script"""
    print("\nüîó Testing simple_sentenceRE3_API.py")
    print("=" * 45)
    
    try:
        # Change to the FOODB_LLM_pipeline directory
        os.chdir("FOODB_LLM_pipeline")
        
        # Import the API script
        sys.path.insert(0, '.')
        import importlib.util
        
        spec = importlib.util.spec_from_file_location(
            "triple_extraction_api", 
            "simple_sentenceRE3_API.py"
        )
        triple_extraction_api = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(triple_extraction_api)
        
        # Load API wrapper
        api_wrapper = triple_extraction_api.load_api_wrapper()
        if not api_wrapper:
            print("‚ùå Failed to load API wrapper")
            os.chdir("..")
            return False
        
        print(f"‚úÖ API wrapper loaded successfully")
        
        # Test triple extraction
        test_sentence = "Resveratrol activates SIRT1 protein."
        test_context = "Resveratrol is found in red wine and has health benefits."
        
        print(f"\nüîó Testing triple extraction...")
        print(f"Sentence: {test_sentence}")
        
        start_time = time.time()
        triples = triple_extraction_api.extract_triples_from_sentence(
            api_wrapper, test_sentence, test_context
        )
        end_time = time.time()
        
        print(f"‚úÖ Extracted in {end_time - start_time:.2f}s")
        print(f"Triples found: {len(triples)}")
        for i, triple in enumerate(triples, 1):
            print(f"  {i}. {triple}")
        
        # Test triple classification
        if triples:
            print(f"\nüè∑Ô∏è Testing triple classification...")
            classification = triple_extraction_api.classify_triple_api(
                api_wrapper, test_context, test_sentence, triples[0]
            )
            
            print(f"Triple: {triples[0]}")
            print(f"Classification: {classification}")
        
        os.chdir("..")
        return True
        
    except Exception as e:
        print(f"‚ùå Error testing triple extraction API: {e}")
        import traceback
        traceback.print_exc()
        os.chdir("..")
        return False

def test_end_to_end_pipeline():
    """Test the complete end-to-end pipeline"""
    print("\nüöÄ Testing End-to-End Pipeline")
    print("=" * 40)
    
    try:
        # Create sample data
        input_file = create_sample_jsonl_data()
        
        # Change to FOODB directory
        os.chdir("FOODB_LLM_pipeline")
        
        # Import and test simple sentence generation
        sys.path.insert(0, '.')
        import importlib.util
        
        spec = importlib.util.spec_from_file_location(
            "simple_sentence_api", 
            "5_LLM_Simple_Sentence_gen_API.py"
        )
        simple_sentence_api = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(simple_sentence_api)
        
        # Process the JSONL file
        output_file = "sample_output_api/processed_test_pipeline_data.jsonl"
        
        print(f"üìÑ Processing JSONL file...")
        print(f"Input: {input_file}")
        print(f"Output: {output_file}")
        
        start_time = time.time()
        simple_sentence_api.process_jsonl_file(
            f"../{input_file}", 
            output_file
        )
        end_time = time.time()
        
        print(f"‚úÖ Processing complete in {end_time - start_time:.2f}s")
        
        # Check results
        if Path(output_file).exists():
            with open(output_file, 'r') as f:
                results = [json.loads(line) for line in f]
            
            print(f"üìä Results:")
            print(f"  Processed entries: {len(results)}")
            
            for i, result in enumerate(results, 1):
                api_results = result.get('api_processing_results', {})
                simple_sentences = api_results.get('simple_sentences', [])
                entities = api_results.get('sentences_with_entities', [])
                
                print(f"  Entry {i}: {len(simple_sentences)} sentences, {len(entities)} with entities")
                
                if simple_sentences:
                    print(f"    Sample sentence: {simple_sentences[0]}")
        
        os.chdir("..")
        return True
        
    except Exception as e:
        print(f"‚ùå Error in end-to-end testing: {e}")
        import traceback
        traceback.print_exc()
        os.chdir("..")
        return False

def test_performance_comparison():
    """Test performance comparison between different approaches"""
    print("\n‚ö° Performance Comparison Test")
    print("=" * 40)
    
    try:
        sys.path.append('FOODB_LLM_pipeline')
        from llm_wrapper import LLMWrapper
        
        wrapper = LLMWrapper()
        
        # Test different types of prompts
        test_cases = [
            {
                "name": "Simple Sentence Generation",
                "prompt": "Convert to simple sentences: Green tea contains EGCG which has antioxidant properties and may help prevent cognitive decline.",
                "expected_time": 1.0
            },
            {
                "name": "Entity Extraction", 
                "prompt": "Extract food compounds and health effects: Curcumin reduces inflammation in arthritis patients.",
                "expected_time": 0.8
            },
            {
                "name": "Triple Extraction",
                "prompt": "Extract [subject, predicate, object] triples: Resveratrol activates SIRT1 protein pathways.",
                "expected_time": 0.6
            }
        ]
        
        print(f"üß™ Running performance tests...")
        
        total_time = 0
        for test_case in test_cases:
            print(f"\nüìù Testing: {test_case['name']}")
            
            start_time = time.time()
            response = wrapper.generate_single(test_case['prompt'], max_tokens=200)
            end_time = time.time()
            
            processing_time = end_time - start_time
            total_time += processing_time
            
            status = "‚úÖ" if processing_time < test_case['expected_time'] else "‚ö†Ô∏è"
            print(f"  {status} Time: {processing_time:.2f}s (expected < {test_case['expected_time']}s)")
            print(f"  üìÑ Response: {response[:100]}...")
        
        print(f"\nüìä Overall Performance:")
        print(f"  Total time: {total_time:.2f}s")
        print(f"  Average per test: {total_time/len(test_cases):.2f}s")
        print(f"  Tests per second: {len(test_cases)/total_time:.2f}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error in performance testing: {e}")
        return False

def main():
    """Run all API script tests"""
    print("üß¨ FOODB API Scripts Testing Suite")
    print("=" * 50)
    
    test_results = []
    
    try:
        # Test 1: Simple sentence API script
        result1 = test_simple_sentence_api_script()
        test_results.append(("Simple Sentence API", result1))
        
        # Test 2: Triple extraction API script
        result2 = test_triple_extraction_api_script()
        test_results.append(("Triple Extraction API", result2))
        
        # Test 3: End-to-end pipeline
        result3 = test_end_to_end_pipeline()
        test_results.append(("End-to-End Pipeline", result3))
        
        # Test 4: Performance comparison
        result4 = test_performance_comparison()
        test_results.append(("Performance Testing", result4))
        
        # Summary
        print(f"\nüìä Test Results Summary:")
        print("=" * 30)
        
        passed = 0
        for test_name, result in test_results:
            status = "‚úÖ PASSED" if result else "‚ùå FAILED"
            print(f"  {test_name}: {status}")
            if result:
                passed += 1
        
        print(f"\nüéØ Overall: {passed}/{len(test_results)} tests passed")
        
        if passed == len(test_results):
            print(f"\nüéâ All tests passed! The FOODB API wrapper is fully functional.")
            print(f"\nüí° You can now:")
            print(f"  ‚Ä¢ Replace local model scripts with API versions")
            print(f"  ‚Ä¢ Process your scientific literature at 10-50x speed")
            print(f"  ‚Ä¢ Run the pipeline without GPU requirements")
            print(f"  ‚Ä¢ Scale to large datasets with API concurrency")
        else:
            print(f"\n‚ö†Ô∏è Some tests failed. Please check the error messages above.")
        
    except Exception as e:
        print(f"‚ùå Testing suite failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
